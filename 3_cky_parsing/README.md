# Saar CoLi Assignment 3: CKY parsing

## Author

Juangui Xu

## Directory Structure

```
│   dataset.py                  -- grammar and sentences loading
│   main.ipynb                  -- main Notebook
│   measure.py                  -- F1 score calculation function
│   parser.py                   -- Parser class, the main part of CKY algorithm
│   parsing_results.txt         -- Parses counts of sentences
│   README.md                   -- This README file
│
└───chart
        backpointer.py          -- BackpointerChar class, used for CKY parsing
        base.py                 -- ChartBase, base class of chart classes
        counting.py             -- CountingChart class, used for CKY counting
        key_only.py             -- KeyOnlyChart class, used for CKY recognizing
        prob_backpointer.py     -- ProbBackpointerChart class, used for Viterbi CKY
        __init__.py
```

## Environments

### Device

- CPU: Snapdragon (TM) 8cx Gen 3 @ 2.69 GHz (8 Cores)
- RAM: 16 GB
- OS: Windows 11 Pro ARM 25H2

### Packages

```
Python==3.13.9
ipykernel==7.0.1
matplotlib==3.10.7
nltk==3.9.2
svgling==0.5.0
```

## Runtime

- On my computer it take ~1s to recognize, ~1s to count, and ~2s to parse all 98 sentences in the dataset.
- However, running NLTK parser to test the grammar will take ~1min. **My algorithm is extremely faster than NLTK**.

## Extra Points

- I implemented a method to convert the NLTK format CFG into Chomsky Normal Form.
- I implemented a method to count the number of parse trees without actually building them.
- I plotted a Sentence Length vs. Speed & Number of Trees curve to compare the efficiency (**in nanosecond precision**).
- I implemented the labeled and unlabeled F1 score calculation and tested it with the example parse trees of sentence `But the concept is workable`.
- I implemented an algorithm to build PCFG based on all the parse trees generated by my parser.
- I implemented a Viterbi CKY parser, and compared it against `nltk.ViterbiParser` on all sentences.

### The Idea of Grammar Normalization

Only rules in `A → B C`, `A -> word` is allowed by Chomsky Normal Form. However, the original `nltk.CFG` may contain rules not follow the format. To normalize the grammar, we need to:
- Deal with the case `A -> B word ...`: create a virtual nonterminal and production `__word__ -> word`, replace the original by ``A -> B __word__ ...``
- Deal with the case `A -> B C D E`: use virtual nontermnials to convert the multi-branch tree into binary, e.g. `A -> __B__C__D__ E`, `__B__C__D__ -> __B__C__ D`, `__B__C__ -> B C`.
- Deal with the case `A -> B -> C -> a`: reduce it as `A -> a`.
- Deal with the case `A -> B -> C -> D E`: reduce it as `A -> D E`.

See `convert_cfg_to_chomsky_normal_form()` in `./dataset.py`.

### The Idea of Counting without Building Trees

In the code of building trees using backpointer (Line 75-83 in `./chart/backpointer.py`):

```python
# Recursively build the left and right subtrees to construct new trees.
trees = trees_cache[cache_key] = list()
for bp in backpointers:
    left_trees = recur(left_idx, bp.mid_idx, bp.left_nonterminal)
    right_trees = recur(bp.mid_idx + 1, right_idx, bp.right_nonterminal)
    trees.extend(
        nltk.Tree(nonterminal, children)
        for children in product(left_trees, right_trees)
    )
```

We can see that for each backpointer $p$, we first build the left subtrees collection $L_p$ and right subtrees collection $R_p$, then use each pair in their Cartesian product $L_p\times R_p$ to build a new tree. Therefore, the total number of subtrees with current node as root would be $\sum_p|L_p\times R_p| = \sum_p|L_p|\times |R_p|$. What we need to do is replacing the backpointers with the countings $|L_p|$ or $|R_p|$, and summing up their product on-the-fly when reducing in CKY.

When reducing $X_1\ X_2\larr A$ with left span $[l,m]$ and right span $[m+1,r]$, the counting on the chart is summed up by: $C(l,r,A) = C(l,r,A) + C(l,m,X_1)\times C(m+1,r,X_2)$. The default values of $C$ are $1$ for each leaf $C(i,i,\bullet)$, and $0$ for others. Note that we dont need backpointers and traceback in this algorithm, the countings could be calculated on-the-fly. We directly output $C(1,N,\sigma)$ as the total number of parses. The algorithm can use $O(N^3)$ time complexity to count the number of parses, without the additional $O(NM)$ for tree-building.

I plotted a Sentence Length vs. Speed & Number of Trees curve to compare and analyse its efficiency against the standard parser which builds all the parse trees.

See `CkyParser.count()` in `./parser.py` and  `CountingChart` in `./chart/counting.py`.

### The Idea of F1-score Calculation

It's hard to compare two trees directly node-by-node.
1. We need to convert the trees into spans firstly, so that each span can represents one node with their structural and label information. (Although a span may unable to uniquely represent a node, this is tolerated by the F1 calculation rules.)
2. Then we can compute the precision and recall, and then F1-score by comparing the spans corresponding to the two trees.

See `tree_to_spans()` and `tree_f1_score()` `./measure.py`.

### The Idea of building PCFG

- To build a PCFG, the first thing is obtaining the probabilities. We can count the number of rules on every trees generated by standard CKY parser as $N(A\rarr X)$, in which $X$ could be $W$ as terminal or $X_L\ X_R$ as pair of nonterminals.
- Then calculate the probabilities by $P(A\rarr X) = \frac{N(A\rarr X)}{\sum N(A \rarr\bullet)}$. After that, use the probabilities and rules to build the PCFG.

See the corresponding section in the main notebook.

### The Idea of Viterbi CKY Parser

- It's extremely easy to implement the Viterbi CKY Parser, just modify the chart used in standard CKY parser to record only the backpointer with max log-probability.
-  When reducing $X_L\ X_R\larr A$ with left span $[l,m]$ and right span $[m+1,r]$, the new log-probability is calculated by: $Y=\text{log}\ P(A\rarr X_1\ X_2)+M(l,m,X_L)+M(m+1,r,X_R)$. If $Y>M(l,r,A)$, then update $M(l,r,A)=Y$ and replace the backpointer by $(m,B,C)$. The default values of $M$ are $\text{log}\ P(A\rarr W_i)$ for each leaf $M(i,i,A)$, and $-\infty$ for others.

See `CkyParser.viterbi()` in `./parser.py` and `ProbBackpointerChart` in `./chart/prob_backpointer.py`.

## Engineering Design Ideas

We can see that the several variants of CKY algorithm share the same backbone logic. The only differences are on the charts. We can modify the original pseudocode as: 

```
Data structure:
    # Ch(i,k) eventually contains {A | A ⇒* wi ... wk-1}, initially all empty. 
    Chart(i,j), eventually contains keys {A | A ⇒* wi ... wk-1}, and map A => abstract_record, initially all as empty dict.

for each i from 1 to n:
    for each production rule A -> w_i:
        # add A to Ch(i, i+1)
        calculate leaf value V by Chart.calc_leaf_value(i,i+1,w_i,A)
        Add key A to Chart(i,i+1) with value V

for each width b from 2 to n:
    for each start position i from 1 to n-b+1:
        for each left width k from 1 to b-1:
            # for each B in Ch(i,i+k) and C in Ch(i+k,i+b):
            for each key B in Chart(i,i+k) and key C in Chart(i+k,i+b):
                for each production rule A -> B C:
                    # add A to Ch(i,i+b)
                    # Add key A with value calculated from left and right to Chart(i,i+b)
                    Chart(i,i+b).reduce(key=A, left=(i,i+k,B), right=(i+k,i+b,C))

Output: Chart(1,n).output(sigma)
```

The data structure `Chart` extends the original `Ch` as a 3D table by `dict`. Beyond `key A`, `dict` enables us to record additional data as `value`. Such data could be countings for CKY counter, backpointers for standard parser, and backpointers with probability for viterbi parser. For CKY recognizer, because it does not need the `value` in `dict`, we can make all `value = None` so that `dict` in `Chart` is equivalent to `set` used in `Ch`.

Therefore, we can design the chart as an abstract data structure class and implement it as different subclasses. By providing the same CKY algorithm backbone with these different chart objects, we are able to avoid rewriting the backbone for every algorithm variants, thereby achieving the goal of efficient code reuse. As we seen in the pseudo-code, the interface of a chart class should contain these methods which will be implemented by the subclasses:
-  `reduce(key=A, left=(i,i+k,B), right=(i+k,i+b,C))`: Add `key A` into the chart and calculate the corresponding `value`, given left span, right span, and production `A -> B C`.
-  `output(sigma)`: Produce algorithm output based on the whole chart, given the root symbol `sigma`.
-  `calc_leaf_value(i,i+1,w_i,A)`: Calculate the leaf record value in the chart, given index `i`, word `w_i`, and nonterminal `A` that `A -> w_i`.

## The Core Algorithm Parts in the Code

Chart classes: I use OOP to implement a base class with several chart subclasses in `./chart/`.
- The base class implemented the access abstraction of chart (`ChartBase.get()`). Note that its compact structure **saves 50% of the space**.
- The base class define three abstract methods used in CKY algorithm defined above, these methods would be implemented by subclasses.
- Subclasses are highly correlated with CKY algorithm variants, hence I will note them in the parser.

CKY backbone:
- `CkyParser._cky_one_sentence()` in `./parser.py` implements the core logic (the 3 loops for Dynamic Programming).
- `CkyParser._reduce()` in `./parser.py` implements the recude action of CKY (2 inner loops). I separated it from `CkyParser._cky_one_sentence` because the depth of loops is too large to make the code ugly.

CKY recognizing: `CkyParser.recognize()` in `./parser.py`. It use `KeyOnlyChart` in `./chart/key_only.py`, which works actually like a set of keys without values. It's worth noting that since the time and space complexity are exactly same, we can actually use CKY counter to replace CKY recognizer.

CKY parsing: `CkyParser.parse()` in `./parser.py`. It use `BackpointerChart` in `./chart/backpointer.py`, which record the list of backpointers as the value of `dict`. Note that I used `list[nltk.Tree]` instead of `set[nltk.ImmutableTree]` to store the trees in building step. The correctness is verified by comparing with the ground truth number of parses in the dataset. Accompanied by an worth noting phenomenon caused by `set`, I discussed why it is correct and better in the last section (Viterbi CKY) of the main notebook. By the way, if this is a mandatory requirement of the assignment, I also impelemented functions to convert `list[nltk.Tree]` into `set[nltk.ImmutableTree]`, in the last section of the notebook.
