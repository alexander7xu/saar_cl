# Saar CoLi Assignment 3: CKY parsing

## Author

Juangui Xu

## Directory Structure

```
│   dataset.py                  -- grammar and sentences loading
│   main.ipynb                  -- main Notebook
│   measure.py                  -- F1 score calculation function
│   parser.py                   -- Parser class, the main part of CKY algorithm
│   parsing_results.txt         -- Parses counts of sentences
│   README.md                   -- This README file
│
└───chart
        backpointer.py          -- BackpointerChar class, used for CKY parsing
        base.py                 -- ChartBase, base class of chart classes
        counting.py             -- CountingChart class, used for CKY counting
        key_only.py             -- KeyOnlyChart class, used for CKY recognizing
        prob_backpointer.py     -- ProbBackpointerChart class, used for Viterbi CKY
        __init__.py
```

## Environments

### Device

- CPU: Snapdragon (TM) 8cx Gen 3 @ 2.69 GHz (8 Cores)
- RAM: 16 GB
- OS: Windows 11 Pro ARM 25H2

### Packages

```
Python==3.13.9
ipykernel==7.0.1
matplotlib==3.10.7
nltk==3.9.2
svgling==0.5.0
```

## Runtime

- On my computer it take ~1s to count and ~2s to parse all 98 sentences in the dataset.
- However, running NLTK parser to get ground truths will take ~1min. **My algorithm is extremely faster than NLTK**.

## Extra Points

- I implemented a method to convert the nltk format CFG into Chomsky Normal Form.
- I implemented a method to count the number of parse trees without actually building them.
- I plotted a Sentence Length vs. Speed & Number of Trees curve to compare the efficiency (**in nanosecond precision**).
- I implemented the labeled and unlabeled F1 score calculation and tested it with the example parse trees of sentence `But the concept is workable`.
- I implemented an algorithm to build PCFG based on all the parse trees from my parser.
- I implemented a Viterbi CKY parser, and compared it against `nltk.ViterbiParser` on all sentences.

### The Idea of Grammar Normalization

Only rules in `A → B C`, `A -> word` is allowed by Chomsky Normal Form. However, the original nltk.CFG may contain rules not follow the format. To normalize the grammar, we need to:
- Deal with the case `A -> B word ...`: create a virtual nonterminal and production `__word__ -> word`, replace the original by ``A -> B __word__ ...``
- Deal with the case `A -> B C D E`: use virtual nontermnials to convert the multi-branch tree into binary, e.g. `A -> __B__C__D__ E`, `__B__C__D__ -> __B__C__ D`, `__B__C__ -> B C`.
- Deal with the case `A -> B -> C -> a`: reduce it as `A -> a`.
- Deal with the case `A -> B -> C -> D E`: reduce it as `A -> D E`.

See `convert_cfg_to_chomsky_normal_form()` in `./dataset.py`.

### The Idea of Counting without Building Trees

In the code of building trees using backpointer (Line 75-83 in `./chart/backpointer.py`):

```python
# Recursively build the left and right subtrees to construct the tree.
trees = trees_buffer[key] = list()
for bp in backpointers:
    left_trees = recur(left_idx, bp.mid_idx, bp.left_nonterminal)
    right_trees = recur(bp.mid_idx + 1, right_idx, bp.right_nonterminal)
    trees.extend(
        nltk.Tree(nonterminal, children)
        for children in product(left_trees, right_trees)
    )
```

We can see that every time after building the left subtrees collection $L$ and right subtrees collection $R$, we use each the pair in the Cartesian product $L\times R$ to build a new tree. Therefore, the number of new trees would be $|L\times R| = |L|\times |R|$. What we need to do is replacing the backpointers with the countings, and calculating them on-the-fly when reducing in CKY.

When reducing $X_1\ X_2\larr A$ with left span $[l,m]$ and right span $[m+1,r]$, the counting on the chart is sum up by: $C(l,r,A) = C(l,r,A) + C(l,m,X_1)\times C(m+1,r,X_2)$. The default values of $C$ are $1$ for leaves $C(i,i,\bullet)$, and $0$ for others.

See `CountingChart` in `./chart/counting.py`

### The Idea of F1-score Calculation

It's hard to compare two trees directly node-by-node.
1. We need to convert the trees into spans firstly, so that each span can represents one node with their structural and label information. (Although a span may unable to uniquely represent a node, this is tolerated by the F1 calculation rules.)
2. Then we can compute the precision and recall, and then F1-score by comparing the spans corresponding to the two trees.

See `tree_to_spans()` and `tree_f1_score()` `./measure.py`

### The Idea of building PCFG

- To build a PCFG, the first thing is obtaining the probabilities. We can count the number of rules on every trees generated by standard CKY parser as $N(A\rarr X)$, in which $X$ could be $W$ as terminal or $X_L\ X_R$ as pair of nonterminals.
- Then calculate the probabilities by $P(A\rarr X) = \frac{N(A\rarr X)}{\sum N(A \rarr\bullet)}$. After that, use the probabilities and rules to build the PCFG.

See the corresponding section in the main notebook.

### The Idea of Viterbi CKY Parser

- It's extremely easy to implement the Viterbi CKY Parser, just modify the chart used in standard CKY parser to record only the backpointer with max log-probability.
-  When reducing $X_1\ X_2\larr A$ with left span $[l,m]$ and right span $[m+1,r]$, the max log-probability is calculated by: $L(l,r,A)=\text{max}(L(l,r,A),\text{log}\ P(A\rarr X_1\ X_2)+L(l,m,X_1)+L(m+1,r,X_2))$. The default values of $L$ are $\text{log}\ P(A\rarr W_i)$ for each leave $L(i,i,A)$, and $-\infty$ for others.

See `ProbBackpointerChart` in `./chart/prob_backpointer.py`.

## Engineering Design Ideas

We can see that severals variants of CKY parser share the same main logic. The only differences are on the charts. We can modify the original pseudocode as: 

```
Data structure:
    # Ch(i,k) eventually contains {A | A ⇒* wi ... wk-1}, initially all empty. 
    Chart(i,j), eventually contains keys {A | A ⇒* wi ... wk-1}, and map A => abstract_record, initially all as empty dict.

for each i from 1 to n:
    for each production rule A -> w_i:
        # add A to Ch(i, i+1)
        calculate leaf value V by Chart.calc_leaf_value(i,i+1,w_i,A)
        Add key A to Chart(i,i+1) with value V

for each width b from 2 to n:
    for each start position i from 1 to n-b+1:
        for each left width k from 1 to b-1:
            # for each B in Ch(i,i+k) and C in Ch(i+k,i+b):
            for each key B in Chart(i,i+k) and key C in Chart(i+k,i+b):
                for each production rule A -> B C:
                    # add A to Ch(i,i+b)
                    # Add key A with value calculated from left and right to Chart(i,i+b)
                    Chart(i,i+b).reduce(key=A, left=(i,i+k,B), right=(i+k,i+b,C))

Output: Chart(1,n).output(sigma)
```

The data structure `Chart` extends the original `Ch` as a 3D table by `dict`. Beyond `key A`, `dict` enables us to record additional data as `value`. Such data could be countings for CKY counter, backpointers for standard parser, and backpointers with probability for viterbi parser. For CKY recognizer, because it does not need the `value` in `dict`, we can make all `value = None` so that `dict` in `Chart` is equivalent to `set` used in `Ch`.

Therefore, we can design the chart as an abstract data structure class and implement it as different subclasses. By providing the same CKY algorithm backbone with these different chart object, we are able to avoid rewriting the backbone for every algorithm variants, thereby achieving the goal of efficient code reuse. As we seen in the pseudo-code, the interface of a chart class should contain these methods which will be implemented by the subclasses:
-  `reduce()`: Add `key A` into the chart and calculate the corresponding `value`.
-  `output()`: Produce algorithm output based on the whole chart.
-  `init_leaf()`: Calculate the leaf record value in the chart.

## The Core Algorithm Parts in the Code

Chart classes: I use OOP to implement a base class with several chart subclasses in `./chart/`.
- The base class implemented the access abstraction of chart (`ChartBase.get()`). Note that its compact structure **saves 50% of the space**.
- The base class define abstract method `ChartBase.reduce()` that need to be implemented by subclasses for main chart actions when reduce in CKY algorithm.
- The base class define abstract method `ChartBase.output()` that need to be implemented by subclasses for producing the final result based on the whole chart.
- The base class define abstract method `ChartBase._calc_leaf_value()` that need to be implemented by subclasses for calculating the leaf record value in CKY algorithm.
- Subclasses are highly correlated with CKY algorithm variants, hence I will note them in the parser.

CKY backbone:
- `CkyParser._cky_one_sentence()` in `./parser.py` implements the core logic (the 3 loops for Dynamic Programming).
- `CkyParser._reduce()` in `./parser.py` implements the recude action of CKY (2 inner loops). I separated it from `CkyParser._cky_one_sentence` because the depth of loops is too large to make the code ugly.

CKY recognizing: `CkyParser.recognize()` in `./parser.py`. It use `KeyOnlyChart` in `./chart/key_only.py`, which works actually like a set of keys without values. It's worth noting that since the time and space complexity are exactly same, we can actually use CKY counting to replace CKY recognizing.

CKY parsing: `CkyParser.parse()` in `./parser.py`. It use `BackpointerChart` in `./chart/backpointer.py`, which record the list of backpointers as the value of `dict`.
